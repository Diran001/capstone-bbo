### Executive summary (6–10 lines)

This capstone solves a constrained black-box optimisation task where the objective functions are unknown and each function can only be queried once per week. I implemented a repeatable Bayesian Optimisation workflow using Gaussian Process surrogate models to propose the next best input for each function. The solution is designed like a small product: clear inputs, deterministic outputs, logging, plots, and a weekly runbook that reduces manual errors. Each iteration updates the training set with the latest portal returns, refits the surrogate, and generates portal-ready submissions in the required format. The approach prioritises sample-efficiency over brute force search, which is the key constraint in this challenge. The repository is structured for auditability, so every recommendation can be traced to a specific week, dataset state, and model fit. This is a pragmatic BO implementation aimed at steady improvement week to week, not a one-off notebook experiment.

### The problem (plain English)

We are trying to find the best input values for several functions, but we are not allowed to see the function equations. We can only “probe” each function by submitting an input and receiving a single output back from the portal. Because we only get one new data point per function per week, we cannot afford trial-and-error. We need a method that learns from a small number of observations and chooses the next test point intelligently. The goal is to move towards the maximum (or minimum) value of each function as quickly as possible, under strict evaluation limits.

### The process (plain English)

Each week follows the same loop. Start with the current dataset for each function (initial points plus all previous weeks). Fit a model that approximates the unknown function from the observed points. Use that model to score candidate inputs, not just by predicted value, but also by uncertainty, so we do not get stuck too early. Pick the single best next point per function, format it exactly as the portal expects, and submit. When the portal returns the eight outputs, append them to the dataset with backups, then repeat for the next week. The notebook produces logs and plots so the decisions are transparent and reproducible.

### Results (what improved, what was best)

The main improvement is decision quality per query. Instead of random guesses or grid search, each weekly point is chosen using a surrogate model and an explicit exploration–exploitation rule. This typically leads to quicker movement towards better regions, especially early on when data is sparse. The strongest behaviour is in functions that are reasonably smooth, where the GP can learn structure from few points and uncertainty estimates are meaningful. The process also improves operational reliability: consistent directory structure, robust parsing of portal files, automated backups, and generation of submission artefacts reduce the risk of week-to-week mistakes. The “best” output of the project is the workflow itself: a defensible and repeatable optimisation pipeline that can be adapted to other expensive-to-evaluate objectives.

### Limitations (noise, adaptive sampling, local optima)

If portal outputs are noisy, the model may chase randomness and overfit, especially with very small sample sizes. Adaptive sampling creates selection bias: the dataset is not an unbiased sample of the input space, so performance estimates based on the observed points can look better than reality. Gaussian Processes can also mislead when the underlying function is highly non-stationary, discontinuous, or has sharp ridges that a smooth kernel struggles to represent. Acquisition functions can get stuck in local optima if exploration is too weak, or waste queries if exploration is too strong. With one point per week, recovery from a bad step is slow, so early hyperparameter choices and bounds matter more than in high-frequency settings. Finally, optimisation success is limited by the allowed domain and the candidate search strategy used to propose points.

### How to use / when not to use

Use this workflow when evaluations are expensive, slow, or limited, and you need the best possible improvement per query. It fits cases like parameter tuning, strategy calibration, simulation-based optimisation, and any setting where you can only run a few experiments. It is appropriate when you need traceability: each proposed point can be explained by model fit, uncertainty, and acquisition logic. Do not use it when you can cheaply evaluate thousands of points, because simpler methods will be faster and often sufficient. Avoid it when the objective is extremely noisy relative to the signal, or when the function is dominated by discontinuities and hard constraints unless you extend the modelling approach. Also avoid treating the surrogate as a “truth model”: it is a decision tool for choosing the next experiment, not a reliable predictor across the full domain.
